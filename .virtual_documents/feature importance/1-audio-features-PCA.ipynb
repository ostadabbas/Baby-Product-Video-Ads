


import pickle
import os
import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.api import OLS
import statsmodels.api as sm
import statsmodels.stats as sts
from scipy import stats
import statsmodels.stats.api as sms
import re

import datetime
date = datetime.datetime.now()
date = date.strftime("%Y.%m.%d")
np.random.seed(0)

def stdz(series: pd.Series):
    """Standardize the given pandas Series"""
    return (series - series.mean())/series.std()
def unitstdz(series:pd.Series):
    return (series - series.min())/(series.max()-series.min())

from imblearn.under_sampling import RandomUnderSampler









rating = pd.read_excel('eyetracking-coordinates-imname.xlsx', sheet_name='video-based')


rating.columns


rating.drop('Shotvariance',axis=1,inplace=True)


rating.head()


def split_ad_id(ad_id):
    if '+' in ad_id[0:5]:
        return ad_id.split('+')[0]
    elif '-' in ad_id[0:5]:
        return ad_id.split('-')[0]
    else:
        return ad_id  # if neither '+' nor '-' is present, return the original ad_id


def lowercase_columns(df):
    df.columns = df.columns.str.lower()
    return df

# Read the first file and convert column names to lowercase
features = pd.read_csv('audio_features.csv')
features = lowercase_columns(features)
features = lowercase_columns(features)
features['VideoNumber'] = features['filename'].apply(split_ad_id)


features.columns


features.tail()


features['VideoNumber'] = features['VideoNumber'].astype(np.int64)


data = features.merge(rating, on='VideoNumber', how='inner')








data['ad_id'] = data['filename'].apply(lambda x: x.replace('.mp3', '.mp4'))





for col in data.columns:
    if data[col].isna().any():
        print(col)


data = data.dropna(how='any')
data = data.replace([np.inf, -np.inf], np.nan).dropna()


features = ['rms_mean', 'rms_std', 'rms_max', 'zcr_mean', 'zcr_std', 'spectral_centroid_mean', 'spectral_centroid_std', 'spectral_bandwidth_mean',
            'spectral_bandwidth_std', 'pitch_mean', 'pitch_std', 'mfcc_1_mean', 'mfcc_1_std', 'mfcc_2_mean', 'mfcc_2_std', 'mfcc_3_mean', 'mfcc_3_std',
            'mfcc_4_mean', 'mfcc_4_std', 'mfcc_5_mean', 'mfcc_5_std', 'mfcc_6_mean', 'mfcc_6_std', 'mfcc_7_mean', 'mfcc_7_std', 'mfcc_8_mean', 'mfcc_8_std', 
            'mfcc_9_mean', 'mfcc_9_std', 'mfcc_10_mean', 'mfcc_10_std', 'mfcc_11_mean', 'mfcc_11_std', 'mfcc_12_mean', 'mfcc_12_std', 'mfcc_13_mean', 
            'mfcc_13_std', 'chroma_1_mean', 'chroma_1_std', 'chroma_2_mean', 'chroma_2_std', 'chroma_3_mean', 'chroma_3_std', 'chroma_4_mean', 'chroma_4_std',
            'chroma_5_mean', 'chroma_5_std', 'chroma_6_mean', 'chroma_6_std', 'chroma_7_mean', 'chroma_7_std', 'chroma_8_mean', 'chroma_8_std', 'chroma_9_mean',
            'chroma_9_std', 'chroma_10_mean', 'chroma_10_std', 'chroma_11_mean', 'chroma_11_std', 'chroma_12_mean', 'chroma_12_std', 'mel_spectrogram_mean', 
            'mel_spectrogram_std']


len(features)





data.head()


def extract_video_number(filename):
    match = re.match(r'(\d+)[+-]', filename)
    return match.group(1) if match else None


dispersion1 = pd.read_json('dispersion_measure0629.json', lines=True)


dispersion1.columns


dispersion1['VideoNumber'] = dispersion1['ad_id'].apply(extract_video_number)
data['VideoNumber'] = data['VideoNumber'].astype(int)
dispersion1['VideoNumber'] = dispersion1['VideoNumber'].astype(int)
data = data.merge(dispersion1[['id', 'std_x', 'std_y', 'combined_std', 'convex_hull_area',
       'convex_hull_area_shapely', 'mean_euclidean_distance', 'VideoNumber']], on='VideoNumber')


#data = data.merge(dispersion[['ad_id','mean_distance','std_distance']], on='ad_id')


data.head(10)


len(data)


data = data.dropna()


len(data)





from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X = data[features]
y = data['convex_hull_area']

# Standardize your features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV






# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Get explained variance ratios
explained_var_ratio = pca.explained_variance_ratio_

# Determine number of components to keep
total_var = 0
n_components = 0
for i, ratio in enumerate(explained_var_ratio):
    total_var += ratio
    if total_var >= 0.975:  # Adjust the threshold as needed
        n_components = i + 1
        break

print(f"Number of principal components selected: {n_components}")

# Apply PCA transformation with selected components
pca_final = PCA(n_components=n_components)
X_selected = pca_final.fit_transform(X_scaled)

# Get selected features (if needed, you can interpret the components to understand which original features contribute the most)
selected_features = X.columns[pca_final.components_.mean(axis=0).argsort()[::-1][:n_components]]
print("Selected features by PCA:", selected_features)


sorted_values = np.sort(selected_features.values)
sorted_values


len(sorted_values)


data['preference'] = data['GroupN(1=low(1-3),2=neutral(4-6),3=high(7-10))']


conditions = [
    (0 < data['PurchaseDesireMean']) & (data['PurchaseDesireMean'] <= 3),
    (3 < data['PurchaseDesireMean']) & (data['PurchaseDesireMean'] <= 6),
    (6 < data['PurchaseDesireMean']) & (data['PurchaseDesireMean'] <= 10)
]

choices = [1, 2, 3]

data['purchase'] = np.select(conditions, choices)





data.columns


data.head()


data.tail()



