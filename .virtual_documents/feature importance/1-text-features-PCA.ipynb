


import pickle
import os
import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.api import OLS
import statsmodels.api as sm
import statsmodels.stats as sts
from scipy import stats
import statsmodels.stats.api as sms


import datetime
date = datetime.datetime.now()
date = date.strftime("%Y.%m.%d")
np.random.seed(0)

def stdz(series: pd.Series):
    """Standardize the given pandas Series"""
    return (series - series.mean())/series.std()
def unitstdz(series:pd.Series):
    return (series - series.min())/(series.max()-series.min())

from imblearn.under_sampling import RandomUnderSampler

import re
def extract_video_number(filename):
    match = re.match(r'(\d+)[+-]', filename)
    return match.group(1) if match else None





rating = pd.read_excel('eyetracking-coordinates-imname.xlsx', sheet_name='video-based')


rating.columns


rating.drop('Shotvariance',axis=1,inplace=True)


rating.head()


text_path = [os.path.join('../NLP',i) for i in os.listdir('../NLP/')]


text_path


def split_ad_id(ad_id):
    if '+' in ad_id[0:5]:
        return ad_id.split('+')[0]
    elif '-' in ad_id[0:5]:
        return ad_id.split('-')[0]
    else:
        return ad_id  # if neither '+' nor '-' is present, return the original ad_id


def lowercase_columns(df):
    df.columns = df.columns.str.lower()
    return df

# Read the first file and convert column names to lowercase
features = pd.read_csv(text_path[0])
features = lowercase_columns(features)

# Read and merge each file into the features dataframe
for path in tqdm.tqdm(text_path[1:-1]):
    df = pd.read_csv(path)
    df = lowercase_columns(df)
    
    common_cols = features.columns.intersection(df.columns).tolist()
    additional_cols = df.columns.difference(features.columns).tolist()

    features = pd.merge(features, df[['filename'] + additional_cols], on='filename', how='inner')

features['VideoNumber'] = features['filename'].apply(split_ad_id)


features.columns


features.tail()


features['VideoNumber'] = features['VideoNumber'].astype(np.int64)


data = features.merge(rating, on='VideoNumber', how='inner')








data['ad_id'] = data['filename'].apply(lambda x: x.replace('.txt', '.mp4'))





for col in data.columns:
    if data[col].isna().any():
        print(col)


data['tone'] = data['tone'].fillna(data['tone'].mean())
data['authentic'] = data['authentic'].fillna(data['authentic'].mean())
data['clout'] = data['clout'].fillna(data['clout'].mean())
data['analytic'] = data['analytic'].fillna(data['analytic'].mean())


data = data.dropna(how='any')
data = data.replace([np.inf, -np.inf], np.nan).dropna()


features = ['wc', 'analytic', 'clout', 'authentic', 'tone', 'wps', 'bigwords', 'dic', 'linguistic', 'function', 'pronoun', 
            'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'det', 'article', 'number', 'prep', 'auxverb', 'adverb', 
            'conj', 'negate', 'verb', 'adj', 'quantity', 'drives', 'affiliation', 'achieve', 'power', 'cognition', 
            'allnone', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certitude', 'differ', 'memory', 'affect',
            'tone_pos', 'tone_neg', 'emotion', 'emo_pos', 'emo_neg', 'emo_anx', 'emo_anger', 'emo_sad', 'swear', 'social',
            'socbehav', 'prosocial', 'polite', 'conflict', 'moral', 'comm', 'socrefs', 'family', 'friend', 'female', 'male',
            'culture', 'politic', 'ethnicity', 'tech', 'lifestyle', 'leisure', 'home', 'work', 'money', 'relig', 'physical',
            'health', 'illness', 'wellness', 'mental', 'substances', 'sexual', 'food', 'death', 'need', 'want', 'acquire', 
            'lack', 'fulfill', 'fatigue', 'reward', 'risk', 'curiosity', 'allure', 'perception', 'attention', 'motion', 
            'space', 'visual', 'auditory', 'feeling', 'time', 'focuspast', 'focuspresent', 'focusfuture', 'conversation', 
            'netspeak', 'assent', 'nonflu', 'filler', 'allpunc', 'period', 'comma', 'qmark', 'exclam', 'apostro', 'otherp', 
            'emoji', 'absolutist', 'agitation', 'dejection', 'accomplishment', 'behavioral_activation', 'breadth', 
            'decisions', 'effort_enjoyment', 'longterm', 'satisfaction', 'structure', 'competence', 'excitement', 
            'not_relevant', 'ruggedness', 'sincerity', 'sophistication', 'highcontroversial', 'lowcontroversial',
            'mediumcontroversial', 'benefit', 'cost', 'creativity_innovation', 'alterations', 'cogs_precogs', 
            'enchantment_emotions', 'fabulations', 'infinity_eternity', 'renewal', 'spiritual_imagery', 'total', 
            'vision', 'agency', 'agentrelatedemotions', 'experience', 'mindoverall', 'patientrelatedemotions', 'security']


len(features)





data.head()


dispersion1 = pd.read_json('dispersion_measure0629.json', lines=True)
dispersion1.head(2)


dispersion1['VideoNumber'] = dispersion1['ad_id'].apply(extract_video_number)
data['VideoNumber'] = data['VideoNumber'].astype(int)
dispersion1['VideoNumber'] = dispersion1['VideoNumber'].astype(int)
data = data.merge(dispersion1[['id', 'std_x', 'std_y', 'combined_std', 'convex_hull_area',
       'convex_hull_area_shapely', 'mean_euclidean_distance', 'VideoNumber']], on='VideoNumber')


data.head(2)


len(data)


data = data.dropna()


len(data)


data.columns





features = ['wc', 'analytic', 'clout', 'authentic', 'tone', 'wps', 'bigwords', 'dic', 'linguistic', 'function', 'pronoun', 
            'ppron', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'det', 'article', 'number', 'prep', 'auxverb', 'adverb', 
            'conj', 'negate', 'verb', 'adj', 'quantity', 'drives', 'affiliation', 'achieve', 'power', 'cognition', 
            'allnone', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certitude', 'differ', 'memory', 'affect',
            'tone_pos', 'tone_neg', 'emotion', 'emo_pos', 'emo_neg', 'emo_anx', 'emo_anger', 'emo_sad', 'swear', 'social',
            'socbehav', 'prosocial', 'polite', 'conflict', 'moral', 'comm', 'socrefs', 'family', 'friend', 'female', 'male',
            'culture', 'politic', 'ethnicity', 'tech', 'lifestyle', 'leisure', 'home', 'work', 'money', 'relig', 'physical',
            'health', 'illness', 'wellness', 'mental', 'substances', 'sexual', 'food', 'death', 'need', 'want', 'acquire', 
            'lack', 'fulfill', 'fatigue', 'reward', 'risk', 'curiosity', 'allure', 'perception', 'attention', 'motion', 
            'space', 'visual', 'auditory', 'feeling', 'time', 'focuspast', 'focuspresent', 'focusfuture', 'conversation', 
            'netspeak', 'assent', 'nonflu', 'filler', 'allpunc', 'period', 'comma', 'qmark', 'exclam', 'apostro', 'otherp', 
            'emoji', 'absolutist', 'agitation', 'dejection', 'accomplishment', 'behavioral_activation', 'breadth', 
            'decisions', 'effort_enjoyment', 'longterm', 'satisfaction', 'structure', 'competence', 'excitement', 
            'not_relevant', 'ruggedness', 'sincerity', 'sophistication', 'highcontroversial', 'lowcontroversial',
            'mediumcontroversial', 'benefit', 'cost', 'creativity_innovation', 'alterations', 'cogs_precogs', 
            'enchantment_emotions', 'fabulations', 'infinity_eternity', 'renewal', 'spiritual_imagery', 'total', 
            'vision', 'agency', 'agentrelatedemotions', 'experience', 'mindoverall', 'patientrelatedemotions', 'security']


from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X = data[features]

# Standardize your features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV






# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Get explained variance ratios
explained_var_ratio = pca.explained_variance_ratio_

# Determine number of components to keep
total_var = 0
n_components = 0
for i, ratio in enumerate(explained_var_ratio):
    total_var += ratio
    if total_var >= 0.975:  # Adjust the threshold as needed
        n_components = i + 1
        break

print(f"Number of principal components selected: {n_components}")

# Apply PCA transformation with selected components
pca_final = PCA(n_components=n_components)
X_selected = pca_final.fit_transform(X_scaled)

# Get selected features (if needed, you can interpret the components to understand which original features contribute the most)
selected_features = X.columns[pca_final.components_.mean(axis=0).argsort()[::-1][:n_components]]
print("Selected features by PCA:", selected_features)


sorted_values = np.sort(selected_features.values)
sorted_values








correlation_matrix = data[list(sorted_values)].corr()
# Define a threshold for high correlation
threshold = 0.9

# Find the index pairs of highly correlated variables
high_corr_var_pairs = [
    (correlation_matrix.columns[i], correlation_matrix.columns[j])
    for i in range(len(correlation_matrix.columns))
    for j in range(i + 1, len(correlation_matrix.columns))
    if correlation_matrix.iloc[i, j] > threshold
]

# Display the highly correlated variable pairs
print("Highly correlated variable pairs:")
for pair in high_corr_var_pairs:
    print(pair)


remove_colinearity_features = ['assent','auditory','breadth','conversation','leisure','netspeak','ppron']
sorted_values = [i for i in sorted_values if i not in remove_colinearity_features]


print(sorted_values)


len(sorted_values)








data['preference'] = data['GroupN(1=low(1-3),2=neutral(4-6),3=high(7-10))']


conditions = [
    (0 < data['PurchaseDesireMean']) & (data['PurchaseDesireMean'] <= 3),
    (3 < data['PurchaseDesireMean']) & (data['PurchaseDesireMean'] <= 6),
    (6 < data['PurchaseDesireMean']) & (data['PurchaseDesireMean'] <= 10)
]

choices = [1, 2, 3]

data['purchase'] = np.select(conditions, choices)





data.columns


data.head()


data.tail()






